{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filename):\n",
    "    data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    data = data[['text', 'subj']]\n",
    "    data['subj'] = data['subj'].apply(lambda subj: subj.split('\\\\'))\n",
    "    data = data.sample(n=10000)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    encoded_subjects = pd.DataFrame(mlb.fit_transform(data.pop('subj')), columns=mlb.classes_, index=data.index)\n",
    "    data = data.join(encoded_subjects)\n",
    "    return data, mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoreis: ['00' 'e1' 'e2' 'e3' 'e4' 'e5' 'e7' 'e8' 'e9' 'f1' 'f2' 'f3' 'f4' 'f5'\n",
      " 'f7' 'f8' 'f9']\n"
     ]
    }
   ],
   "source": [
    "train, categories = prepare_data('learn.txt')\n",
    "test, _ = prepare_data('test.txt')\n",
    "print('Categoreis: {}'.format(categories))\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1</td>\n",
       "      <td>COMPUTERS; ELECTRONICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e2</td>\n",
       "      <td>ASTRONOMY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e3</td>\n",
       "      <td>BIOLOGY; MEDICAL SCIENCES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e4</td>\n",
       "      <td>GEOGRAPHY; GEOPHYSICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e5</td>\n",
       "      <td>GEOLOGY; EARTH SCIENCES; MINES AND MINING INDU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                                        description\n",
       "0   e1                             COMPUTERS; ELECTRONICS\n",
       "1   e2                                          ASTRONOMY\n",
       "2   e3                          BIOLOGY; MEDICAL SCIENCES\n",
       "3   e4                              GEOGRAPHY; GEOPHYSICS\n",
       "4   e5  GEOLOGY; EARTH SCIENCES; MINES AND MINING INDU..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects = pd.read_csv('subjects.txt', sep=\"\\t\", header=None, names=['code', 'desc_rus', 'description'])[['code', 'description']]\n",
    "subjects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "X_test = test.text\n",
    "Y_train = train[categories]\n",
    "Y_test = test[categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max input length is:  411\n"
     ]
    }
   ],
   "source": [
    "xLengths = [len(word_tokenize(x)) for x in X_train]\n",
    "h = sorted(xLengths)  #sorted lengths\n",
    "maxLength =h[len(h)-1]\n",
    "print(\"max input length is: \",maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70% cover input sequence length up to 144\n"
     ]
    }
   ],
   "source": [
    "maxLength = h[int(len(h) * 0.70)]\n",
    "print(\"70% cover input sequence length up to\",maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 50564\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 200000\n",
    "input_tokenizer = Tokenizer(max_vocab_size)\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "     #matthews_correlation\n",
    "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "     y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "     y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "     tp = K.sum(y_pos * y_pred_pos)\n",
    "     tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "     fp = K.sum(y_neg * y_pred_pos)\n",
    "     fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "     numerator = (tp * tn - fp * fn)\n",
    "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    " \n",
    " \n",
    "     return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 144, 256)          12944384  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 144, 256)          393984    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                4369      \n",
      "=================================================================\n",
      "Total params: 13,736,721\n",
      "Trainable params: 13,736,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "num_categories = len(categories)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_vocab_size, embedding_dim,input_length = maxLength))\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "model.add(Dense(num_categories, activation='sigmoid'))\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[precision, recall ,f1])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 144, 256)          21129728  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 144, 256)          655616    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300)               668400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 18)                5418      \n",
      "=================================================================\n",
      "Total params: 22,459,162\n",
      "Trainable params: 22,459,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, MaxPooling1D, Dropout, Conv1D, LSTM\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_categories = len(categories)\n",
    "embedding_dim = 256\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_vocab_size, embedding_dim,input_length = maxLength))\n",
    "model.add(Conv1D(filters=256, kernel_size=10, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=9))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(num_categories, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[precision, recall ,f1])\n",
    "print(model.summary())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 152s 17ms/step - loss: 0.2806 - acc: 0.8977 - val_loss: 0.2011 - val_acc: 0.9301\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 146s 16ms/step - loss: 0.2010 - acc: 0.9298 - val_loss: 0.1973 - val_acc: 0.9326\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 150s 17ms/step - loss: 0.1873 - acc: 0.9362 - val_loss: 0.1739 - val_acc: 0.9419\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 139s 15ms/step - loss: 0.1679 - acc: 0.9438 - val_loss: 0.1609 - val_acc: 0.9438\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 138s 15ms/step - loss: 0.1554 - acc: 0.9483 - val_loss: 0.1552 - val_acc: 0.9475\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 137s 15ms/step - loss: 0.1460 - acc: 0.9516 - val_loss: 0.1501 - val_acc: 0.9499\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 137s 15ms/step - loss: 0.1341 - acc: 0.9555 - val_loss: 0.1410 - val_acc: 0.9529\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 160s 18ms/step - loss: 0.1238 - acc: 0.9588 - val_loss: 0.1373 - val_acc: 0.9540\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 153s 17ms/step - loss: 0.1195 - acc: 0.9604 - val_loss: 0.1354 - val_acc: 0.9536\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 171s 19ms/step - loss: 0.1135 - acc: 0.9622 - val_loss: 0.1376 - val_acc: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0d98ebd68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(totalX, Y_train, validation_split=0.1, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalX_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(totalX_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(totalX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00145262, 0.05488734, 0.01588299, 0.06202881, 0.01076351,\n",
       "       0.07601661, 0.00561291, 0.01605309, 0.1513212 , 0.12173117,\n",
       "       0.12067848, 0.03418665, 0.00514579, 0.11736965, 0.03171026,\n",
       "       0.00121652, 0.27438316], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(predicted):\n",
    "    for j, prob in enumerate(row):\n",
    "        if prob >= 0.2:\n",
    "            predicted[i][j]=1\n",
    "        else:\n",
    "            predicted[i][j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          00       0.00      0.00      0.00        79\n",
      "          e1       0.43      0.69      0.53       940\n",
      "          e2       0.14      0.07      0.09       178\n",
      "          e3       0.69      0.85      0.76      1403\n",
      "          e4       0.29      0.52      0.37       269\n",
      "          e5       0.60      0.05      0.09       321\n",
      "          e7       0.00      0.00      0.00         9\n",
      "          e8       0.21      0.73      0.32       275\n",
      "          e9       0.03      0.00      0.01       201\n",
      "          f1       0.23      0.21      0.22       355\n",
      "          f2       0.00      0.00      0.00       173\n",
      "          f3       0.23      0.32      0.27       654\n",
      "          f4       0.00      0.00      0.00       112\n",
      "          f5       0.57      0.87      0.68      2175\n",
      "          f7       0.76      0.91      0.82      3971\n",
      "          f8       0.00      0.00      0.00        24\n",
      "          f9       0.24      0.17      0.20       561\n",
      "\n",
      "   micro avg       0.55      0.69      0.61     11700\n",
      "   macro avg       0.26      0.32      0.26     11700\n",
      "weighted avg       0.54      0.69      0.59     11700\n",
      " samples avg       0.63      0.72      0.64     11700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(Y_test), predicted, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5406510742857371, 0.6894871794871795, 0.5916607989219517, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(np.array(Y_test), predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "\n",
    "POS_WEIGHT = 10  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \n",
    "    #Weighted binary crossentropy between an output tensor \n",
    "    #and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    #for the positive targets.\n",
    "\n",
    "    #Combination of the following functions:\n",
    "    #* keras.losses.binary_crossentropy\n",
    "    #* keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    #* tf.nn.weighted_cross_entropy_with_logits\n",
    "    \n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "    \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
