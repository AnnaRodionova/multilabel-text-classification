{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filename):\n",
    "    data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    data = data[['text', 'subj']]\n",
    "    data['subj'] = data['subj'].apply(lambda subj: subj.split('\\\\'))\n",
    "    data = data.sample(n=10000)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    encoded_subjects = pd.DataFrame(mlb.fit_transform(data.pop('subj')), columns=mlb.classes_, index=data.index)\n",
    "    data = data.join(encoded_subjects)\n",
    "    return data, mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoreis: ['00' 'e1' 'e2' 'e3' 'e4' 'e5' 'e7' 'e8' 'e9' 'f1' 'f2' 'f3' 'f4' 'f5'\n",
      " 'f7' 'f8' 'f9']\n"
     ]
    }
   ],
   "source": [
    "train, categories = prepare_data('learn.txt')\n",
    "test, _ = prepare_data('test.txt')\n",
    "print('Categoreis: {}'.format(categories))\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1</td>\n",
       "      <td>COMPUTERS; ELECTRONICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e2</td>\n",
       "      <td>ASTRONOMY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e3</td>\n",
       "      <td>BIOLOGY; MEDICAL SCIENCES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e4</td>\n",
       "      <td>GEOGRAPHY; GEOPHYSICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e5</td>\n",
       "      <td>GEOLOGY; EARTH SCIENCES; MINES AND MINING INDU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                                        description\n",
       "0   e1                             COMPUTERS; ELECTRONICS\n",
       "1   e2                                          ASTRONOMY\n",
       "2   e3                          BIOLOGY; MEDICAL SCIENCES\n",
       "3   e4                              GEOGRAPHY; GEOPHYSICS\n",
       "4   e5  GEOLOGY; EARTH SCIENCES; MINES AND MINING INDU..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects = pd.read_csv('subjects.txt', sep=\"\\t\", header=None, names=['code', 'desc_rus', 'description'])[['code', 'description']]\n",
    "subjects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "X_test = test.text\n",
    "Y_train = train[categories]\n",
    "Y_test = test[categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max input length is:  612\n"
     ]
    }
   ],
   "source": [
    "xLengths = [len(word_tokenize(x)) for x in X_train]\n",
    "h = sorted(xLengths)  #sorted lengths\n",
    "maxLength =h[len(h)-1]\n",
    "print(\"max input length is: \",maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70% cover input sequence length up to 145\n"
     ]
    }
   ],
   "source": [
    "maxLength = h[int(len(h) * 0.70)]\n",
    "print(\"70% cover input sequence length up to\",maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 51644\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 200000\n",
    "input_tokenizer = Tokenizer(max_vocab_size)\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "num_categories = len(categories)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_vocab_size, embedding_dim,input_length = maxLength))\n",
    "    model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "    model.add(GRU(256, dropout=0.9))\n",
    "    model.add(Dense(num_categories, activation='sigmoid'))\n",
    "    #model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[precision, recall ,f1])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 145, 256)          13220864  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 145, 256)          393984    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                4369      \n",
      "=================================================================\n",
      "Total params: 14,013,201\n",
      "Trainable params: 14,013,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "batch_size = [ 80, 100]\n",
    "epochs = [10, 50]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(totalX, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 152s 17ms/step - loss: 0.2806 - acc: 0.8977 - val_loss: 0.2011 - val_acc: 0.9301\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 146s 16ms/step - loss: 0.2010 - acc: 0.9298 - val_loss: 0.1973 - val_acc: 0.9326\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 150s 17ms/step - loss: 0.1873 - acc: 0.9362 - val_loss: 0.1739 - val_acc: 0.9419\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 139s 15ms/step - loss: 0.1679 - acc: 0.9438 - val_loss: 0.1609 - val_acc: 0.9438\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 138s 15ms/step - loss: 0.1554 - acc: 0.9483 - val_loss: 0.1552 - val_acc: 0.9475\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 137s 15ms/step - loss: 0.1460 - acc: 0.9516 - val_loss: 0.1501 - val_acc: 0.9499\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 137s 15ms/step - loss: 0.1341 - acc: 0.9555 - val_loss: 0.1410 - val_acc: 0.9529\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 160s 18ms/step - loss: 0.1238 - acc: 0.9588 - val_loss: 0.1373 - val_acc: 0.9540\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 153s 17ms/step - loss: 0.1195 - acc: 0.9604 - val_loss: 0.1354 - val_acc: 0.9536\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 171s 19ms/step - loss: 0.1135 - acc: 0.9622 - val_loss: 0.1376 - val_acc: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0d98ebd68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(totalX, Y_train, validation_split=0.1, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalX_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(totalX_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(totalX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00145262, 0.05488734, 0.01588299, 0.06202881, 0.01076351,\n",
       "       0.07601661, 0.00561291, 0.01605309, 0.1513212 , 0.12173117,\n",
       "       0.12067848, 0.03418665, 0.00514579, 0.11736965, 0.03171026,\n",
       "       0.00121652, 0.27438316], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(predicted):\n",
    "    for j, prob in enumerate(row):\n",
    "        if prob >= 0.2:\n",
    "            predicted[i][j]=1\n",
    "        else:\n",
    "            predicted[i][j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          00       0.00      0.00      0.00        79\n",
      "          e1       0.43      0.69      0.53       940\n",
      "          e2       0.14      0.07      0.09       178\n",
      "          e3       0.69      0.85      0.76      1403\n",
      "          e4       0.29      0.52      0.37       269\n",
      "          e5       0.60      0.05      0.09       321\n",
      "          e7       0.00      0.00      0.00         9\n",
      "          e8       0.21      0.73      0.32       275\n",
      "          e9       0.03      0.00      0.01       201\n",
      "          f1       0.23      0.21      0.22       355\n",
      "          f2       0.00      0.00      0.00       173\n",
      "          f3       0.23      0.32      0.27       654\n",
      "          f4       0.00      0.00      0.00       112\n",
      "          f5       0.57      0.87      0.68      2175\n",
      "          f7       0.76      0.91      0.82      3971\n",
      "          f8       0.00      0.00      0.00        24\n",
      "          f9       0.24      0.17      0.20       561\n",
      "\n",
      "   micro avg       0.55      0.69      0.61     11700\n",
      "   macro avg       0.26      0.32      0.26     11700\n",
      "weighted avg       0.54      0.69      0.59     11700\n",
      " samples avg       0.63      0.72      0.64     11700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(Y_test), predicted, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5406510742857371, 0.6894871794871795, 0.5916607989219517, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(np.array(Y_test), predicted, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
