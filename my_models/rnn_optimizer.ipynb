{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    def prepare_data(filename):\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "        data = data[['text', 'subj']]\n",
    "        data['subj'] = data['subj'].apply(lambda subj: subj.split('\\\\'))\n",
    "        data = data.sample(n=10000)\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        encoded_subjects = pd.DataFrame(mlb.fit_transform(data.pop('subj')), columns=mlb.classes_, index=data.index)\n",
    "        data = data.join(encoded_subjects)\n",
    "        return data, mlb.classes_\n",
    "\n",
    "    train, categories = prepare_data('learn.txt')\n",
    "    test, _ = prepare_data('test.txt')\n",
    "    \n",
    "    subjects = pd.read_csv('subjects.txt', sep=\"\\t\", header=None, names=['code', 'desc_rus', 'description'])[['code', 'description']]\n",
    "    \n",
    "    X_train = train.text\n",
    "    X_test = test.text\n",
    "    Y_train = train[categories]\n",
    "    Y_test = test[categories]\n",
    "    \n",
    "    xLengths = [len(word_tokenize(x)) for x in X_train]\n",
    "    h = sorted(xLengths)  #sorted lengths\n",
    "\n",
    "    \n",
    "    maxLength = h[int(len(h) * 0.70)]\n",
    "    \n",
    "    max_vocab_size = 200000\n",
    "    input_tokenizer = Tokenizer(max_vocab_size)\n",
    "    input_tokenizer.fit_on_texts(X_train)\n",
    "    input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "    \n",
    "    X_train = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength))\n",
    "    X_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength))\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    embedding_dim = 256\n",
    "    num_categories = len(categories)\n",
    " \n",
    "    pool_length = 4\n",
    "    lstm_output_size = 100\n",
    "    batch_size = 200\n",
    "    nb_epoch = 10\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_vocab_size, embedding_dim,input_length = maxLength))\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Conv1D({{choice([64, 128])}},\n",
    "                            {{choice([6, 8])}},\n",
    "                            padding='valid',\n",
    "                            activation='relu',\n",
    "                            strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_length))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(num_categories))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "              \n",
    "    print('Train...')\n",
    "    result = model.fit(X_train, Y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=nb_epoch,\n",
    "              verbose=2,\n",
    "              validation_split=0.1)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.advanced_activations import LeakyReLU\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MultiLabelBinarizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.pipeline import Pipeline\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.embeddings import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.recurrent import GRU\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Conv1D': hp.choice('Conv1D', [64, 128]),\n",
      "        'Conv1D_1': hp.choice('Conv1D_1', [6, 8]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: def prepare_data(filename):\n",
      "   3:     data = pd.read_csv(filename, sep=\"\\t\")\n",
      "   4:     data = data[['text', 'subj']]\n",
      "   5:     data['subj'] = data['subj'].apply(lambda subj: subj.split('\\\\'))\n",
      "   6:     data = data.sample(n=10000)\n",
      "   7:     mlb = MultiLabelBinarizer()\n",
      "   8:     encoded_subjects = pd.DataFrame(mlb.fit_transform(data.pop('subj')), columns=mlb.classes_, index=data.index)\n",
      "   9:     data = data.join(encoded_subjects)\n",
      "  10:     return data, mlb.classes_\n",
      "  11: \n",
      "  12: train, categories = prepare_data('learn.txt')\n",
      "  13: test, _ = prepare_data('test.txt')\n",
      "  14: \n",
      "  15: subjects = pd.read_csv('subjects.txt', sep=\"\\t\", header=None, names=['code', 'desc_rus', 'description'])[['code', 'description']]\n",
      "  16: \n",
      "  17: X_train = train.text\n",
      "  18: X_test = test.text\n",
      "  19: Y_train = train[categories]\n",
      "  20: Y_test = test[categories]\n",
      "  21: \n",
      "  22: xLengths = [len(word_tokenize(x)) for x in X_train]\n",
      "  23: h = sorted(xLengths)  #sorted lengths\n",
      "  24: \n",
      "  25: \n",
      "  26: maxLength = h[int(len(h) * 0.70)]\n",
      "  27: \n",
      "  28: max_vocab_size = 200000\n",
      "  29: input_tokenizer = Tokenizer(max_vocab_size)\n",
      "  30: input_tokenizer.fit_on_texts(X_train)\n",
      "  31: input_vocab_size = len(input_tokenizer.word_index) + 1\n",
      "  32: \n",
      "  33: X_train = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength))\n",
      "  34: X_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength))\n",
      "  35: \n",
      "  36: \n",
      "  37: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     embedding_dim = 256\n",
      "   5:     num_categories = len(categories)\n",
      "   6:  \n",
      "   7:     pool_length = 4\n",
      "   8:     lstm_output_size = 100\n",
      "   9:     batch_size = 200\n",
      "  10:     nb_epoch = 10\n",
      "  11:     \n",
      "  12:     model = Sequential()\n",
      "  13:     model.add(Embedding(input_vocab_size, embedding_dim,input_length = maxLength))\n",
      "  14:     \n",
      "  15:     model.add(Dropout(space['Dropout']))\n",
      "  16:     model.add(Conv1D(space['Conv1D'],\n",
      "  17:                             space['Conv1D_1'],\n",
      "  18:                             padding='valid',\n",
      "  19:                             activation='relu',\n",
      "  20:                             strides=1))\n",
      "  21:     model.add(MaxPooling1D(pool_size=pool_length))\n",
      "  22:     model.add(LSTM(lstm_output_size))\n",
      "  23:     model.add(Dense(num_categories))\n",
      "  24:     model.add(Activation('sigmoid'))\n",
      "  25: \n",
      "  26:     model.compile(loss='binary_crossentropy',\n",
      "  27:                   optimizer='adam',\n",
      "  28:                   metrics=['accuracy'])\n",
      "  29:               \n",
      "  30:     print('Train...')\n",
      "  31:     result = model.fit(X_train, Y_train, \n",
      "  32:               batch_size=batch_size, \n",
      "  33:               epochs=nb_epoch,\n",
      "  34:               verbose=2,\n",
      "  35:               validation_split=0.1)\n",
      "  36:     \n",
      "  37:     #get the highest validation accuracy of the training epochs\n",
      "  38:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  39:     print('Best validation acc of epoch:', validation_acc)\n",
      "  40:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  41: \n",
      "Train...\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 29s - loss: 0.3239 - acc: 0.9085 - val_loss: 0.2040 - val_acc: 0.9289\n",
      "Epoch 2/10\n",
      " - 26s - loss: 0.1987 - acc: 0.9301 - val_loss: 0.1986 - val_acc: 0.9289\n",
      "Epoch 3/10\n",
      " - 26s - loss: 0.1866 - acc: 0.9342 - val_loss: 0.1856 - val_acc: 0.9357\n",
      "Epoch 4/10\n",
      " - 26s - loss: 0.1695 - acc: 0.9415 - val_loss: 0.1777 - val_acc: 0.9396\n",
      "Epoch 5/10\n",
      " - 26s - loss: 0.1514 - acc: 0.9496 - val_loss: 0.1660 - val_acc: 0.9465\n",
      "Epoch 6/10\n",
      " - 26s - loss: 0.1293 - acc: 0.9610 - val_loss: 0.1534 - val_acc: 0.9474\n",
      "Epoch 7/10\n",
      " - 26s - loss: 0.1122 - acc: 0.9658 - val_loss: 0.1505 - val_acc: 0.9488\n",
      "Epoch 8/10\n",
      " - 26s - loss: 0.1014 - acc: 0.9687 - val_loss: 0.1519 - val_acc: 0.9475\n",
      "Epoch 9/10\n",
      " - 26s - loss: 0.0928 - acc: 0.9712 - val_loss: 0.1532 - val_acc: 0.9486\n",
      "Epoch 10/10\n",
      " - 26s - loss: 0.0844 - acc: 0.9738 - val_loss: 0.1557 - val_acc: 0.9486\n",
      "Best validation acc of epoch: 0.9487650394439697\n",
      "Train...\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 31s - loss: 0.3352 - acc: 0.9032 - val_loss: 0.2037 - val_acc: 0.9289\n",
      "Epoch 2/10\n",
      " - 29s - loss: 0.1984 - acc: 0.9301 - val_loss: 0.1982 - val_acc: 0.9289\n",
      "Epoch 3/10\n",
      " - 29s - loss: 0.1866 - acc: 0.9332 - val_loss: 0.1871 - val_acc: 0.9330\n",
      "Epoch 4/10\n",
      " - 32s - loss: 0.1719 - acc: 0.9380 - val_loss: 0.1817 - val_acc: 0.9350\n",
      "Epoch 5/10\n",
      " - 29s - loss: 0.1519 - acc: 0.9485 - val_loss: 0.1616 - val_acc: 0.9461\n",
      "Epoch 6/10\n",
      " - 29s - loss: 0.1235 - acc: 0.9626 - val_loss: 0.1495 - val_acc: 0.9497\n",
      "Epoch 7/10\n",
      " - 30s - loss: 0.1054 - acc: 0.9675 - val_loss: 0.1499 - val_acc: 0.9501\n",
      "Epoch 8/10\n",
      " - 29s - loss: 0.0926 - acc: 0.9699 - val_loss: 0.1518 - val_acc: 0.9494\n",
      "Epoch 9/10\n",
      " - 29s - loss: 0.0825 - acc: 0.9732 - val_loss: 0.1563 - val_acc: 0.9477\n",
      "Epoch 10/10\n",
      " - 29s - loss: 0.0734 - acc: 0.9776 - val_loss: 0.1595 - val_acc: 0.9468\n",
      "Best validation acc of epoch: 0.9500591516494751\n",
      "Train...\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 28s - loss: 0.3203 - acc: 0.9020 - val_loss: 0.2043 - val_acc: 0.9289\n",
      "Epoch 2/10\n",
      " - 26s - loss: 0.1992 - acc: 0.9301 - val_loss: 0.2001 - val_acc: 0.9289\n",
      "Epoch 3/10\n",
      " - 26s - loss: 0.1885 - acc: 0.9355 - val_loss: 0.1869 - val_acc: 0.9378\n",
      "Epoch 4/10\n",
      " - 26s - loss: 0.1685 - acc: 0.9444 - val_loss: 0.1747 - val_acc: 0.9409\n",
      "Epoch 5/10\n",
      " - 26s - loss: 0.1454 - acc: 0.9548 - val_loss: 0.1635 - val_acc: 0.9474\n",
      "Epoch 6/10\n",
      " - 26s - loss: 0.1250 - acc: 0.9645 - val_loss: 0.1590 - val_acc: 0.9481\n",
      "Epoch 7/10\n",
      " - 26s - loss: 0.1101 - acc: 0.9678 - val_loss: 0.1585 - val_acc: 0.9482\n",
      "Epoch 8/10\n",
      " - 26s - loss: 0.0978 - acc: 0.9702 - val_loss: 0.1577 - val_acc: 0.9487\n",
      "Epoch 9/10\n",
      " - 26s - loss: 0.0879 - acc: 0.9733 - val_loss: 0.1598 - val_acc: 0.9479\n",
      "Epoch 10/10\n",
      " - 26s - loss: 0.0791 - acc: 0.9762 - val_loss: 0.1633 - val_acc: 0.9466\n",
      "Best validation acc of epoch: 0.9487062454223633\n",
      "Train...\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 42s - loss: 0.3125 - acc: 0.9118 - val_loss: 0.2032 - val_acc: 0.9289\n",
      "Epoch 2/10\n",
      " - 40s - loss: 0.1971 - acc: 0.9314 - val_loss: 0.1955 - val_acc: 0.9361\n",
      "Epoch 3/10\n",
      " - 40s - loss: 0.1777 - acc: 0.9437 - val_loss: 0.1791 - val_acc: 0.9408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      " - 40s - loss: 0.1556 - acc: 0.9490 - val_loss: 0.1716 - val_acc: 0.9448\n",
      "Epoch 5/10\n",
      " - 40s - loss: 0.1347 - acc: 0.9585 - val_loss: 0.1641 - val_acc: 0.9461\n",
      "Epoch 6/10\n",
      " - 39s - loss: 0.1159 - acc: 0.9650 - val_loss: 0.1587 - val_acc: 0.9468\n",
      "Epoch 7/10\n",
      " - 40s - loss: 0.1011 - acc: 0.9690 - val_loss: 0.1555 - val_acc: 0.9491\n",
      "Epoch 8/10\n",
      " - 40s - loss: 0.0886 - acc: 0.9727 - val_loss: 0.1567 - val_acc: 0.9490\n",
      "Epoch 9/10\n",
      " - 41s - loss: 0.0767 - acc: 0.9774 - val_loss: 0.1561 - val_acc: 0.9501\n",
      "Epoch 10/10\n",
      " - 40s - loss: 0.0652 - acc: 0.9827 - val_loss: 0.1586 - val_acc: 0.9488\n",
      "Best validation acc of epoch: 0.9501179695129395\n",
      "Train...\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 32s - loss: 0.3019 - acc: 0.9153 - val_loss: 0.2040 - val_acc: 0.9289\n",
      "Epoch 2/10\n",
      " - 30s - loss: 0.2004 - acc: 0.9301 - val_loss: 0.2025 - val_acc: 0.9289\n",
      "Epoch 3/10\n",
      " - 29s - loss: 0.1969 - acc: 0.9309 - val_loss: 0.1942 - val_acc: 0.9309\n",
      "Epoch 4/10\n",
      " - 29s - loss: 0.1817 - acc: 0.9417 - val_loss: 0.1824 - val_acc: 0.9405\n",
      "Epoch 5/10\n",
      " - 29s - loss: 0.1676 - acc: 0.9457 - val_loss: 0.1755 - val_acc: 0.9421\n",
      "Epoch 6/10\n",
      " - 29s - loss: 0.1547 - acc: 0.9500 - val_loss: 0.1642 - val_acc: 0.9478\n",
      "Epoch 7/10\n",
      " - 30s - loss: 0.1376 - acc: 0.9568 - val_loss: 0.1550 - val_acc: 0.9496\n",
      "Epoch 8/10\n",
      " - 29s - loss: 0.1240 - acc: 0.9619 - val_loss: 0.1477 - val_acc: 0.9512\n",
      "Epoch 9/10\n",
      " - 29s - loss: 0.1128 - acc: 0.9652 - val_loss: 0.1535 - val_acc: 0.9494\n",
      "Epoch 10/10\n",
      " - 29s - loss: 0.1050 - acc: 0.9679 - val_loss: 0.1543 - val_acc: 0.9500\n",
      "Best validation acc of epoch: 0.9512356162071228\n",
      "Evalutation of best performing model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_5_input to have shape (143,) but got array with shape (144,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e651b9e744ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best performing model chosen hyper-parameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_5_input to have shape (143,) but got array with shape (144,)"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,data=data,algo=tpe.suggest,max_evals=5,trials=Trials(),notebook_name='rnn_optimizer')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Conv1D': 0, 'Conv1D_1': 1, 'Dropout': 0.8713141896816126}\n"
     ]
    }
   ],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
