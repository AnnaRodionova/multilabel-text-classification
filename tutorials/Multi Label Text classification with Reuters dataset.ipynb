{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "# Set Numpy random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './reuters/'\n",
    "\n",
    "sgml_number_of_files = 22\n",
    "sgml_file_name_template = 'reut2-{}.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents and categories\n",
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0], \n",
    "                                  0])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = DataFrame(data=category_data, columns=['Name', 'Type', 'Newslines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_frequencies(categories):\n",
    "    for category in categories:\n",
    "        idx = news_categories[news_categories.Name == category].index[0]\n",
    "        f = news_categories.get_value(idx, 'Newslines')\n",
    "        news_categories.set_value(idx, 'Newslines', f+1)\n",
    "    \n",
    "def to_category_vector(categories, target_categories):\n",
    "    vector = zeros(len(target_categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those are the top 20 categories we will use for the classification\n",
    "selected_categories = ['pl_usa', 'to_earn', 'to_acq', 'pl_uk', 'pl_japan', 'pl_canada', 'to_money-fx',\n",
    " 'to_crude', 'to_grain', 'pl_west-germany', 'to_trade', 'to_interest',\n",
    " 'pl_france', 'or_ec', 'pl_brazil', 'to_wheat', 'to_ship', 'pl_australia',\n",
    " 'to_corn', 'pl_china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "document_X = []\n",
    "document_Y = []\n",
    "\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  after removing the cwd from sys.path.\n",
      "/home/anna/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n",
      "Reading file: reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "# Iterate all files\n",
    "for i in range(sgml_number_of_files):\n",
    "    file_name = sgml_file_name_template.format(str(i).zfill(3))\n",
    "    print('Reading file: %s' % file_name)\n",
    "    \n",
    "    with open(data_folder + file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(), \"lxml\")\n",
    "        \n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "            \n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "            \n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].text)).replace('reuter\\n&#3;', '')\n",
    "            document_body = unescape(document_body)\n",
    "            \n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "            \n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "                \n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "                \n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "                \n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "                \n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "                \n",
    "            # Create new document    \n",
    "            update_frequencies(document_categories)\n",
    "            \n",
    "            document_X.append(document_body)\n",
    "            document_Y.append(to_category_vector(document_categories, selected_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Newslines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>pl_usa</td>\n",
       "      <td>Places</td>\n",
       "      <td>12542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>to_earn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>3987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to_acq</td>\n",
       "      <td>Topics</td>\n",
       "      <td>2448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>pl_uk</td>\n",
       "      <td>Places</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>pl_japan</td>\n",
       "      <td>Places</td>\n",
       "      <td>1138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>pl_canada</td>\n",
       "      <td>Places</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>to_money-fx</td>\n",
       "      <td>Topics</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>to_crude</td>\n",
       "      <td>Topics</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>to_grain</td>\n",
       "      <td>Topics</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>pl_west-germany</td>\n",
       "      <td>Places</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>to_trade</td>\n",
       "      <td>Topics</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>to_interest</td>\n",
       "      <td>Topics</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>pl_france</td>\n",
       "      <td>Places</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>or_ec</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>pl_brazil</td>\n",
       "      <td>Places</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>to_wheat</td>\n",
       "      <td>Topics</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>to_ship</td>\n",
       "      <td>Topics</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>pl_australia</td>\n",
       "      <td>Places</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>to_corn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>pl_china</td>\n",
       "      <td>Places</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name           Type  Newslines\n",
       "296           pl_usa         Places      12542\n",
       "35           to_earn         Topics       3987\n",
       "0             to_acq         Topics       2448\n",
       "293            pl_uk         Places       1489\n",
       "219         pl_japan         Places       1138\n",
       "166        pl_canada         Places       1104\n",
       "73       to_money-fx         Topics        801\n",
       "28          to_crude         Topics        634\n",
       "45          to_grain         Topics        628\n",
       "302  pl_west-germany         Places        567\n",
       "126         to_trade         Topics        552\n",
       "55       to_interest         Topics        513\n",
       "191        pl_france         Places        469\n",
       "587            or_ec  Organizations        349\n",
       "158        pl_brazil         Places        332\n",
       "130         to_wheat         Topics        306\n",
       "108          to_ship         Topics        305\n",
       "145     pl_australia         Places        270\n",
       "19           to_corn         Topics        254\n",
       "172         pl_china         Places        223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select top 20 categories (by number of newslines)\n",
    "news_categories.sort_values(by='Newslines', ascending=False, inplace=True)\n",
    "# Selected categories\n",
    "selected_categories = np.array(news_categories[\"Name\"].head(20))\n",
    "num_categories = 20\n",
    "news_categories.head(num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average yen cd rates fall in latest week\n",
      "    tokyo, feb 27 - average interest rates on yen certificates\n",
      "of deposit, cd, fell to 4.27 pct in the week ended february 25\n",
      "from 4.32 pct the previous week, the bank of japan said.\n",
      "    new rates (previous in brackets), were -\n",
      "    average cd rates all banks 4.27 pct (4.32)\n",
      "    money market certificate, mmc, ceiling rates for the week\n",
      "starting from march 2          3.52 pct (3.57)\n",
      "    average cd rates of city, trust and long-term banks\n",
      "    less than 60 days          4.33 pct (4.32)\n",
      "    60-90 days                 4.13 pct (4.37)\n",
      "    average cd rates of city, trust and long-term banks\n",
      "    90-120 days             4.35 pct (4.30)\n",
      "    120-150 days            4.38 pct (4.29)\n",
      "    150-180 days            unquoted (unquoted)\n",
      "    180-270 days            3.67 pct (unquoted)\n",
      "    over 270 days           4.01 pct (unquoted)\n",
      "    average yen bankers' acceptance rates of city, trust and\n",
      "long-term banks\n",
      "    30 to less than 60 days unquoted (4.13)\n",
      "    60-90 days              unquoted (unquoted)\n",
      "    90-120 days             unquoted (unquoted)\n",
      " reuter\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the input and output data\n",
    "print(document_X[220])\n",
    "print(document_Y[220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anna/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-615f70ede32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstrip_special_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^A-Za-z0-9 ]+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleanUpSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# Clean up the data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "stop_words = set(nltk.download('stopwords').words(\"english\"))\n",
    "\n",
    "def cleanUpSentence(r, stop_words = None):\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = re.sub(strip_special_chars, \"\", r.lower())\n",
    "    if stop_words is not None:\n",
    "        words = word_tokenize(r)\n",
    "        filtered_sentence = []\n",
    "        for w in words:\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        return \" \".join(filtered_sentence)\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
